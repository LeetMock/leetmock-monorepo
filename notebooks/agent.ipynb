{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local LangGraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "url = \"https://leetmock-ts-fa225f46565756e7b0567441810f232f.default.us.langgraph.app\"\n",
    "client = get_client(url=url)\n",
    "\n",
    "# Using the graph deployed with the name \"agent\"\n",
    "assistant_id = \"code-mock-staged-v1\"\n",
    "\n",
    "# create thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant = await client.assistants.create(graph_id=assistant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_graph.code_mock_staged_v1.graph import create_compiled_graph\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# graph = create_graph().compile()\n",
    "graph = create_compiled_graph()\n",
    "config = { \"configurable\": {\"thread_id\": \"1\", \"session_id1\": \"abc\"} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: List[str] = []\n",
    "\n",
    "\n",
    "graph = StateGraph(State).compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in graph.astream(\n",
    "    input={\n",
    "        \"messages\": [\"Hi\"],\n",
    "        \"event\": \"reminder\",\n",
    "        # \"trigger\": True,\n",
    "    },\n",
    "    config=config,\n",
    "    stream_mode=[\"values\"],\n",
    "):\n",
    "    print(chunk)\n",
    "\n",
    "async for chunk in graph.astream(\n",
    "    input={\n",
    "        \"messages\": [\"Hi\"],\n",
    "        # \"event\": \"user_message\"\n",
    "    },\n",
    "    config=config,\n",
    "    stream_mode=[\"values\"],\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load.load import load, loads\n",
    "from langchain_core.load.dump import dumpd, dumps\n",
    "from langgraph.types import StateSnapshot\n",
    "\n",
    "state = graph.get_state(config=config)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state = dumpd(state)\n",
    "dumped_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state = load(dumped_state, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_state_snapshot = StateSnapshot(*loaded_state)\n",
    "loaded_state_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot == state, dumpd(loaded_state_snapshot) == dumpd(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot.values == state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(graph.get_state_history(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_history = dumps(history)\n",
    "dumped_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history = loads(dumped_history, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_history_snapshot = [StateSnapshot(*h) for h in loaded_history]\n",
    "loaded_history_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history_snapshot == history, dumpd(loaded_history_snapshot) == dumpd(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state_byte = dumped_state.encode()\n",
    "dumped_history_byte = dumped_history.encode()\n",
    "print(f\"State: {len(dumped_state_byte) / 1024:.2f} KB\")\n",
    "print(f\"State History: {len(dumped_history_byte) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = graph.get_graph().draw_mermaid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"graph.md\", \"w\") as f:\n",
    "    f.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, PrivateAttr\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    _a: str = PrivateAttr(init=False)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"base class init\")\n",
    "        self._a = \"a\"\n",
    "\n",
    "\n",
    "class User2(User):\n",
    "\n",
    "    c: str = Field(...)\n",
    "\n",
    "\n",
    "user = User2(name=\"Charlie\", age=20, c=\"c\")\n",
    "print(user)  # Output: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "CACHE_CONFIG = {\"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            template=\"You are a {role} assistant.\" * 1000,\n",
    "            additional_kwargs=CACHE_CONFIG,\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            template=\"{input}\",\n",
    "            additional_kwargs=CACHE_CONFIG,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatAnthropic(\n",
    "    model_name=\"claude-3-5-haiku-20241022\",\n",
    "    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},  # type: ignore\n",
    ")\n",
    "\n",
    "openai = ChatOpenAI(name=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | chat\n",
    "# chain.invoke(\n",
    "#     {\n",
    "#         \"role\": \"helpful\",\n",
    "#         \"input\": \"What is second page of the shakespeare?\",\n",
    "#     }\n",
    "# ).dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"text\": \"You are a helpful assistant.\" * 1000,\n",
    "                    \"type\": \"text\",\n",
    "                    **CACHE_CONFIG,\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"text\": \"What is second page of the shakespeare?\",\n",
    "                    \"type\": \"text\",\n",
    "                    **CACHE_CONFIG,\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"text\": \"What is first page of the shakespeare?\",\n",
    "                    \"type\": \"text\",\n",
    "                    **CACHE_CONFIG,\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    ").dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "json.dumps(\n",
    "    {\n",
    "        \"context\": \"test\",\n",
    "        \"name\": \"test\",\n",
    "        \"time\": time.time(),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langgraph.func import entrypoint, task\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
    "val: list[int] = []\n",
    "count: int = 5000\n",
    "\n",
    "def _inc():\n",
    "    val.append(len(val))\n",
    "    return val\n",
    "\n",
    "@task(name=\"async_inc\")\n",
    "async def ainc(topic: str):\n",
    "    return _inc()\n",
    "\n",
    "\n",
    "@task(name=\"async_thread_inc\")\n",
    "async def thread_ainc(topic: str):\n",
    "    return await asyncio.to_thread(_inc)\n",
    "\n",
    "\n",
    "@task(name=\"sync_inc\")\n",
    "def inc():\n",
    "    return _inc()\n",
    "\n",
    "# Build workflow\n",
    "@entrypoint()\n",
    "async def async_workflow(topic: str, previous: str):\n",
    "    rs = [ainc(topic) for _ in range(count)]\n",
    "\n",
    "    return asyncio.gather(*rs)\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "async def async_thread_workflow(topic: str, previous: str):\n",
    "    rs = [thread_ainc(topic) for _ in range(count)]\n",
    "\n",
    "    return await asyncio.gather(*rs)\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def sync_workflow(topic: str, previous: str):\n",
    "    rs = [inc() for _ in range(count)]\n",
    "\n",
    "    return [r.result() for r in rs]\n",
    "\n",
    "\n",
    "def check_final_val(val: list[int]):\n",
    "    assert len(val) == count\n",
    "    for i, v in enumerate(val):\n",
    "        assert v == i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "# Invoke\n",
    "async for step in sync_workflow.astream(\"cat\", stream_mode=\"updates\"):\n",
    "    ...\n",
    "\n",
    "check_final_val(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "# Invoke\n",
    "async for step in async_thread_workflow.astream(\"cat\", stream_mode=\"updates\"):\n",
    "    ...\n",
    "\n",
    "check_final_val(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "# Invoke\n",
    "async for step in async_workflow.astream(\"cat\", stream_mode=\"updates\"):\n",
    "    ...\n",
    "\n",
    "check_final_val(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "\n",
    "await asyncio.gather(async_workflow.ainvoke(\"cat\"), async_workflow.ainvoke(\"cat\"), async_workflow.ainvoke(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "context = {\n",
    "    \"silent\": False,\n",
    "}\n",
    "\n",
    "def chunk_tokenize(text: str):\n",
    "    return text.split(\" \")\n",
    "\n",
    "@task\n",
    "async def should_silent(text: str):\n",
    "    await asyncio.sleep(3)\n",
    "    return context[\"silent\"]\n",
    "\n",
    "@task\n",
    "async def invoke_agent(text: str):\n",
    "    silent_fut = should_silent(text)\n",
    "\n",
    "    no_silent = False\n",
    "    accumulated_text = \"\"\n",
    "    for token in chunk_tokenize(text):\n",
    "        if silent_fut.done() and not no_silent:\n",
    "            if silent_fut.result():\n",
    "                print(\"[silent]\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"[done]\")\n",
    "                print(accumulated_text, end=\"\")\n",
    "                no_silent = True\n",
    "\n",
    "        await asyncio.sleep(0.1)\n",
    "        if not no_silent:\n",
    "            # print(f\"[accumulating] {token}\")\n",
    "            accumulated_text += token + \" \"\n",
    "        else:\n",
    "            print(token + \" \", end=\"\")\n",
    "\n",
    "    await silent_fut\n",
    "    if silent_fut.result():\n",
    "        print(\"[silent]\")\n",
    "    else:\n",
    "        print(\"[done]\")\n",
    "        print(accumulated_text, end=\"\")\n",
    "\n",
    "@task\n",
    "async def set_silent(silent: bool):\n",
    "    context[\"silent\"] = silent\n",
    "\n",
    "@entrypoint()\n",
    "async def agent_workflow(obj: dict):\n",
    "    await set_silent(obj[\"silent\"])\n",
    "    await invoke_agent(obj[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very long story\n",
    "story = \"\"\"\\\n",
    "Short-term memory¶\n",
    "State management using the previous parameter and optionally using the entrypoint.final primitive can be used to implement short term memory.\n",
    "\n",
    "Please see the following how-to guides for more details:\n",
    "\n",
    "How to add thread-level persistence (functional API): Shows how to add thread-level persistence to a functional API workflow and implements a simple chatbot.\n",
    "Long-term memory¶\n",
    "long-term memory allows storing information across different thread ids. This could be useful for learning information about a given user in one conversation and using it in another.\n",
    "\n",
    "Please see the following how-to guides for more details:\n",
    "\n",
    "How to add cross-thread persistence (functional API): Shows how to add cross-thread persistence to a functional API workflow and implements a simple chatbot.\n",
    "Workflows¶\n",
    "Workflows and agent guide for more examples of how to build workflows using the Functional API.\n",
    "Agents¶\n",
    "How to create a React agent from scratch (Functional API): Shows how to create a simple React agent from scratch using the functional API.\n",
    "How to build a multi-agent network: Shows how to build a multi-agent network using the functional API.\n",
    "How to add multi-turn conversation in a multi-agent application (functional API): allow an end-user to engage in a multi-turn conversation with one or more agents.\n",
    "\"\"\"\n",
    "\n",
    "await agent_workflow.ainvoke({\"text\": story, \"silent\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=1, max_completion_tokens=2)\n",
    "llm.invoke(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Start workflow\n",
      "End workflow\n",
      "Start long task\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "End long task\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx: dict = {}\n",
    "\n",
    "@task\n",
    "async def long_task(t: int):\n",
    "    print(f\"Start long task {t}\")\n",
    "    await asyncio.sleep(t)\n",
    "    ctx[\"done\"] = True\n",
    "    print(f\"End long task {t}\")\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "async def workflow(t: int):\n",
    "    print(f\"Start workflow {t}\")\n",
    "    long_task(t)\n",
    "    print(f\"End workflow {t}\")\n",
    "    return\n",
    "\n",
    "async def loop():\n",
    "    while not ctx.get(\"done\", False):\n",
    "        print(ctx)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "await asyncio.gather(loop(), workflow.ainvoke(5), workflow.ainvoke(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
