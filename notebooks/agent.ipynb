{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local LangGraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "url = \"https://leetmock-ts-fa225f46565756e7b0567441810f232f.default.us.langgraph.app\"\n",
    "client = get_client(url=url)\n",
    "\n",
    "# Using the graph deployed with the name \"agent\"\n",
    "assistant_id = \"code-mock-staged-v1\"\n",
    "\n",
    "# create thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant = await client.assistants.create(graph_id=assistant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_graph.code_mock_staged_v1.graph import create_compiled_graph\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# graph = create_graph().compile()\n",
    "graph = create_compiled_graph()\n",
    "config = { \"configurable\": {\"thread_id\": \"1\", \"session_id1\": \"abc\"} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in graph.astream(\n",
    "    input={\n",
    "        \"messages\": [\"Hi\"],\n",
    "        \"event\": \"reminder\",\n",
    "        # \"trigger\": True,\n",
    "    },\n",
    "    config=config,\n",
    "    stream_mode=[\"values\"],\n",
    "):\n",
    "    print(chunk)\n",
    "\n",
    "async for chunk in graph.astream(\n",
    "    input={\n",
    "        \"messages\": [\"Hi\"],\n",
    "        # \"event\": \"user_message\"\n",
    "    },\n",
    "    config=config,\n",
    "    stream_mode=[\"values\"],\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load.load import load, loads\n",
    "from langchain_core.load.dump import dumpd, dumps\n",
    "from langgraph.types import StateSnapshot\n",
    "\n",
    "state = graph.get_state(config=config)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state = dumps(state)\n",
    "dumped_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state = loads(dumped_state, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_state_snapshot = StateSnapshot(*loaded_state)\n",
    "loaded_state_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot == state, dumpd(loaded_state_snapshot) == dumpd(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot.values == state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(graph.get_state_history(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_history = dumps(history)\n",
    "dumped_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history = loads(dumped_history, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_history_snapshot = [StateSnapshot(*h) for h in loaded_history]\n",
    "loaded_history_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history_snapshot == history, dumpd(loaded_history_snapshot) == dumpd(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state_byte = dumped_state.encode()\n",
    "dumped_history_byte = dumped_history.encode()\n",
    "print(f\"State: {len(dumped_state_byte) / 1024:.2f} KB\")\n",
    "print(f\"State History: {len(dumped_history_byte) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = graph.get_graph().draw_mermaid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"graph.md\", \"w\") as f:\n",
    "    f.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.v1 import BaseModel, Field, PrivateAttr\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    _a: str = PrivateAttr(...)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"base class init\")\n",
    "        self._a = \"a\"\n",
    "\n",
    "\n",
    "class User2(User):\n",
    "\n",
    "    c: str = Field(...)\n",
    "\n",
    "\n",
    "user = User2(name=\"Charlie\", age=20, c=\"c\")\n",
    "print(user)  # Output: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byin/Documents/GitHub/leetmock-monorepo/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "CACHE_CONFIG = {\"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            template=\"You are a {role} assistant.\" * 1000,\n",
    "            additional_kwargs=CACHE_CONFIG,\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            template=\"{input}\",\n",
    "            additional_kwargs=CACHE_CONFIG,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat = ChatAnthropic(\n",
    "    model_name=\"claude-3-5-haiku-20241022\",\n",
    "    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},  # type: ignore\n",
    ")\n",
    "\n",
    "openai = ChatOpenAI(name=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | chat\n",
    "# chain.invoke(\n",
    "#     {\n",
    "#         \"role\": \"helpful\",\n",
    "#         \"input\": \"What is second page of the shakespeare?\",\n",
    "#     }\n",
    "# ).dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"William Shakespeare was an English playwright and poet who lived in the late 16th and early 17th centuries. His works are widely regarded as some of the greatest in the English language. Shakespeare's first page would vary depending on the specific edition or collection of his works that you are referring to, as his plays and poems have been published and organized in many different ways over the centuries. If you have a specific edition in mind, I can help you locate the first page of Shakespeare's works in that edition.\",\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 103,\n",
       "   'prompt_tokens': 5034,\n",
       "   'total_tokens': 5137,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-3.5-turbo-0125',\n",
       "  'system_fingerprint': None,\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-45e7d131-b99b-42b2-bba8-5f4f1306e8ab-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 5034,\n",
       "  'output_tokens': 103,\n",
       "  'total_tokens': 5137}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"text\": \"You are a helpful assistant.\" * 1000,\n",
    "                    \"type\": \"text\",\n",
    "                    **CACHE_CONFIG,\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"text\": \"What is second page of the shakespeare?\",\n",
    "                    \"type\": \"text\",\n",
    "                    **CACHE_CONFIG,\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"text\": \"What is first page of the shakespeare?\",\n",
    "                    \"type\": \"text\",\n",
    "                    **CACHE_CONFIG,\n",
    "                }\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    ").dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"context\": \"test\", \"name\": \"test\", \"time\": 1731480108.65847}'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "json.dumps(\n",
    "    {\n",
    "        \"context\": \"test\",\n",
    "        \"name\": \"test\",\n",
    "        \"time\": time.time(),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
