{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local LangGraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "url = \"https://leetmock001-76a7d6889223553c93a96358909dd6e3.default.us.langgraph.app\"\n",
    "client = get_client(url=url)\n",
    "\n",
    "# Using the graph deployed with the name \"agent\"\n",
    "assistant_id = \"template\"\n",
    "\n",
    "# create thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "print(thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant = await client.assistants.create(graph_id=assistant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from agent_graph.template.graph import create_graph\n",
    "\n",
    "\n",
    "graph = create_graph()\n",
    "async def print_time():\n",
    "    start_t = time.time()\n",
    "    async for chunk in client.runs.stream(\n",
    "        thread[\"thread_id\"],\n",
    "        assistant_id,\n",
    "        input={\"messages\": [\"HI\"], \"event\": \"user_message\"},\n",
    "        stream_mode=[\"updates\"],\n",
    "        multitask_strategy=\"interrupt\",\n",
    "    ):\n",
    "        # tags = chunk.data.get(\"tags\", [])\n",
    "        # if \"chatbot\" not in tags:\n",
    "        #     continue\n",
    "\n",
    "        # event = chunk.data.get(\"event\", None)\n",
    "        # if event != \"on_chat_model_stream\":\n",
    "        #     continue\n",
    "\n",
    "        # content = chunk.data.get(\"data\", {}).get(\"chunk\", {}).get(\"content\", \"\")\n",
    "        return time.time() - start_t\n",
    "        print(\"-\" * 100)\n",
    "        print(chunk)\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "async def print_time_local():\n",
    "    start_t = time.time()\n",
    "    async for chunk in graph.astream_events(\n",
    "        input={\"messages\": [\"HI\"], \"trigger\": True},\n",
    "        version=\"v2\",\n",
    "    ):\n",
    "        return time.time() - start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = []\n",
    "for _ in range(200):\n",
    "    t = await print_time_local()\n",
    "    times.append(t)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make interval to be 0.01\n",
    "import numpy as np\n",
    "\n",
    "# Determine the range of the data\n",
    "min_time = min(times)\n",
    "max_time = max(times)\n",
    "\n",
    "# Create bin edges with 0.01 intervals\n",
    "# plt.xticks(np.arange(min_time, max_time + 0.2, 0.2))\n",
    "\n",
    "plt.hist(times)\n",
    "plt.xlabel(\"Time taken (s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of time taken to respond\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(times) / len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_graph.code_mock_staged_v1.graph import create_graph\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "graph = create_graph(stateful=False)\n",
    "config = { \"configurable\": {\"thread_id\": \"1\" } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in graph.astream_events(\n",
    "    input={\n",
    "        \"messages\": [\n",
    "            \"Hi, my name is Brian. I am a software engineer working at Meta. My background is in computer science and I have a passion for building scalable and efficient systems. My previous projects are in building scalable and efficient systems, as well as building scalable and efficient systems. My goal is to build scalable and efficient systems.\"\n",
    "        ],\n",
    "        # \"event\": \"user_message\"\n",
    "        # \"trigger\": True,\n",
    "    },\n",
    "    # config=config,\n",
    "    version=\"v2\",\n",
    "):\n",
    "    pass\n",
    "\n",
    "# async for chunk in graph.astream(\n",
    "#     input={\n",
    "#         \"messages\": [\"Hi, my name is Brian. I am a software engineer working at Meta. My background is in computer science and I have a passion for building scalable and efficient systems. My previous projects are in building scalable and efficient systems, as well as building scalable and efficient systems. My goal is to build scalable and efficient systems.\"],\n",
    "#         # \"event\": \"user_message\"\n",
    "#         \"trigger\": True\n",
    "#     },\n",
    "#     config=config,\n",
    "#     stream_mode=[\"updates\"],\n",
    "# ):\n",
    "#     print(chunk)\n",
    "# list(graph.get_state_history(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await graph.ainvoke({ \"messages\": [\"Hi, my name is Brian. I am a software engineer working at Meta. My background is in computer science and I have a passion for building scalable and efficient systems. My previous projects are in building scalable and efficient systems, as well as building scalable and efficient systems. My goal is to build scalable and efficient systems.\"], \"trigger\": True }, config=config)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load.load import load, loads\n",
    "from langchain_core.load.dump import dumpd, dumps\n",
    "from langgraph.types import StateSnapshot\n",
    "\n",
    "state = graph.get_state(config=config)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state = dumps(state)\n",
    "dumped_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state = loads(dumped_state, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_state_snapshot = StateSnapshot(*loaded_state)\n",
    "loaded_state_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot == state, dumpd(loaded_state_snapshot) == dumpd(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot.values == state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(graph.get_state_history(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_history = dumps(history)\n",
    "dumped_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history = loads(dumped_history, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_history_snapshot = [StateSnapshot(*h) for h in loaded_history]\n",
    "loaded_history_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history_snapshot == history, dumpd(loaded_history_snapshot) == dumpd(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state_byte = dumped_state.encode()\n",
    "dumped_history_byte = dumped_history.encode()\n",
    "print(f\"State: {len(dumped_state_byte) / 1024:.2f} KB\")\n",
    "print(f\"State History: {len(dumped_history_byte) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
