{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local LangGraph Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "url = \"https://leetmock001-76a7d6889223553c93a96358909dd6e3.default.us.langgraph.app\"\n",
    "client = get_client(url=url)\n",
    "\n",
    "# Using the graph deployed with the name \"agent\"\n",
    "assistant_id = \"template\"\n",
    "\n",
    "# create thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "print(thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant = await client.assistants.create(graph_id=assistant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from agent_graph.template.graph import create_graph\n",
    "\n",
    "\n",
    "graph = create_graph()\n",
    "async def print_time():\n",
    "    start_t = time.time()\n",
    "    async for chunk in client.runs.stream(\n",
    "        thread[\"thread_id\"],\n",
    "        assistant_id,\n",
    "        input={\"messages\": [\"HI\"], \"event\": \"user_message\"},\n",
    "        stream_mode=[\"updates\"],\n",
    "        multitask_strategy=\"interrupt\",\n",
    "    ):\n",
    "        # tags = chunk.data.get(\"tags\", [])\n",
    "        # if \"chatbot\" not in tags:\n",
    "        #     continue\n",
    "\n",
    "        # event = chunk.data.get(\"event\", None)\n",
    "        # if event != \"on_chat_model_stream\":\n",
    "        #     continue\n",
    "\n",
    "        # content = chunk.data.get(\"data\", {}).get(\"chunk\", {}).get(\"content\", \"\")\n",
    "        return time.time() - start_t\n",
    "        print(\"-\" * 100)\n",
    "        print(chunk)\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "async def print_time_local():\n",
    "    start_t = time.time()\n",
    "    async for chunk in graph.astream_events(\n",
    "        input={\"messages\": [\"HI\"], \"trigger\": True},\n",
    "        version=\"v2\",\n",
    "    ):\n",
    "        return time.time() - start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = []\n",
    "for _ in range(200):\n",
    "    t = await print_time_local()\n",
    "    times.append(t)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make interval to be 0.01\n",
    "import numpy as np\n",
    "\n",
    "# Determine the range of the data\n",
    "min_time = min(times)\n",
    "max_time = max(times)\n",
    "\n",
    "# Create bin edges with 0.01 intervals\n",
    "# plt.xticks(np.arange(min_time, max_time + 0.2, 0.2))\n",
    "\n",
    "plt.hist(times)\n",
    "plt.xlabel(\"Time taken (s)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of time taken to respond\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(times) / len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_graph.code_mock_staged_v1.graph import create_compiled_graph, create_graph\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# graph = create_graph().compile()\n",
    "graph = create_compiled_graph()\n",
    "config = { \"configurable\": {\"thread_id\": \"1\" } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('updates', {'on_trigger': None})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='Hey', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' there', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='!', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=\" I'm\", id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' Brian', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=',', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' and', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=\" I've\", id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' been', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' working', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' at', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' Roblox', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' for', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' the', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' past', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' five', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' years', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=',', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' mainly', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' in', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' Gener', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='ative', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' AI', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' and', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' machine', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' learning', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='.', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=\" It's\", id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' great', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' to', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' meet', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' you', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='!', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' How', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' about', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' you', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' tell', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' me', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' a', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' bit', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' about', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content=' yourself', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='?', id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('custom', {'id': 'assistant', 'data': AIMessageChunk(content='', response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1'}, id='run-c1dd3929-d353-4013-b8ae-b0218116a713')})\n",
      "('updates', {<StageTypes.INTRO: 'intro'>: {'messages': [HumanMessage(content='Hi', id='65c5909c-1c37-4432-9d7c-0074e7fe170e'), HumanMessage(content='(User just joined the call, please welcome and introduce yourself:)', id='6a6ed32d-e9f3-4d89-8944-9dcd51d3f622'), HumanMessage(content='Hi', id='20bad30f-8a38-40e8-9de6-b4b445ca95e8'), AIMessage(content=\"Hey there! I'm Brian, and I've been working at Roblox for the past five years, mainly in Generative AI and machine learning. It's great to meet you! How about you tell me a bit about yourself?\", response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1'}, id='run-c1dd3929-d353-4013-b8ae-b0218116a713')], 'steps': {<StageTypes.INTRO: 'intro'>: [Step(name='introduce_self', description='Concisely introduce yourself, brief talk about your background (feel free to make things up about your background, just be consistent throughout the interview).', done_definition='Interviewer has finished introducing themselves.', required=True), Step(name='ask_background', description='Ask the candidate about their background and experience.', done_definition=\"Interviewer has finished asking about the candidate's background and experience.\", required=True), Step(name='ask_goals', description='Ask the candidate about their career goals.', done_definition=\"Interviewer has finished asking about the candidate's career goals.\", required=True), Step(name='discuss_projects', description=\"Discuss the candidate's past projects and their role in them. Remember to praise interviewee on their achievement.\", done_definition=\"Interviewer has finished discussing the candidate's past projects and their role in them.\", required=True)], <StageTypes.CODING: 'coding'>: [Step(name='describe_problem', description='Describe the problem to the candidate.', done_definition='Interviewer has finished describing the problem.', required=True), Step(name='prompt_coding', description='Let the candidate start writing the code to solve the problem.', done_definition='Interviewer has finished prompting the candidate to start writing the code, or the candidate has started writing the code without prompting.', required=True), Step(name='monitor_coding', description=\"Monitor the candidate's coding progress, respond accordingly if they need help, clarification, or if they are stuck and need hints.\", done_definition=\"Monitoring step isn't done until candidate has finished writing the complete code solution (not necessarily passed all test cases).\", required=True), Step(name='prompt_explain_code', description='Let the candidate explain their code and the approach they took to solve the problem.', done_definition='Interviewer has finished prompting the candidate to explain their code, or the candidate has finished explaining their code without prompting.', required=True), Step(name='prompt_test_case', description='Let the candidate write the test cases for their code.', done_definition='Interviewer has finished prompting the candidate to write the test cases, or the candidate has finished writing the test cases without prompting.', required=True), Step(name='prompt_optimization', description=\"If candidate's code can be optimized, ask them if they can think of a better solution\", done_definition='Interviewer has finished prompting the candidate to optimize their code.')], <StageTypes.EVAL: 'eval'>: [Step(name='tell_overall_perf', description='Tell candidate how they did in the interview.', done_definition='Interviewer has finished telling candidate how they did in the interview.', required=True), Step(name='tell_strength', description='Tell candidate what they did well and what they could improve on.', done_definition='Interviewer has finished telling candidate what they did well and what they could improve on.', required=True), Step(name='suggest_improve', description='Give candidate suggestions on how to improve their coding skills.', done_definition='Interviewer has finished giving candidate suggestions on how to improve their coding skills.', required=True), Step(name='ask_question_back', description='Ask candidate if they have any questions for you.', done_definition='Interviewer has finished asking candidate if they have any questions for you.', required=True), Step(name='answer_user_question', description='Answer related questions user might have', done_definition='Interviewer has finished answering related questions user might have')]}, 'signals': {<StageTypes.INTRO: 'intro'>: [Signal(name='candidate_intro_done', description='The candidate has introduced themselves about their background.'), Signal(name='candidate_goals_done', description='The candidate has finished talking about their career goals.'), Signal(name='candidate_projects_done', description='The candidate has finished talking about their past projects and their role in them.')], <StageTypes.CODING: 'coding'>: [Signal(name='clarifying_questions', description='The candidate asks clarifying questions about the problem.'), Signal(name='thought_process', description='The candidate explains their high-level thought process for solving the problem.'), Signal(name='code_finished', description='The candidate has finished writing the code.'), Signal(name='passed_all_tests', description='The candidate has passed all test cases.')], <StageTypes.EVAL: 'eval'>: [Signal(name='user_questions', description='User has asked questions about the interview process or the candidate.')]}}})\n",
      "('updates', {'stage_tracker': {'messages': [HumanMessage(content='Hi', id='65c5909c-1c37-4432-9d7c-0074e7fe170e'), HumanMessage(content='(User just joined the call, please welcome and introduce yourself:)', id='6a6ed32d-e9f3-4d89-8944-9dcd51d3f622'), HumanMessage(content='Hi', id='20bad30f-8a38-40e8-9de6-b4b445ca95e8'), AIMessage(content=\"Hey there! I'm Brian, and I've been working at Roblox for the past five years, mainly in Generative AI and machine learning. It's great to meet you! How about you tell me a bit about yourself?\", response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1'}, id='run-c1dd3929-d353-4013-b8ae-b0218116a713'), AIMessage(content='<thinking>\\nI have finished the step(s): introduce_self, ask_background. Now I will move on to the next step: ask_goals.\\n</thinking>', id='0665b169-e280-4b84-8703-7431fbe4de96')], 'current_stage': <StageTypes.INTRO: 'intro'>, 'steps': {<StageTypes.INTRO: 'intro'>: [Step(name='introduce_self', description='Concisely introduce yourself, brief talk about your background (feel free to make things up about your background, just be consistent throughout the interview).', done_definition='Interviewer has finished introducing themselves.', required=True), Step(name='ask_background', description='Ask the candidate about their background and experience.', done_definition=\"Interviewer has finished asking about the candidate's background and experience.\", required=True), Step(name='ask_goals', description='Ask the candidate about their career goals.', done_definition=\"Interviewer has finished asking about the candidate's career goals.\", required=True), Step(name='discuss_projects', description=\"Discuss the candidate's past projects and their role in them. Remember to praise interviewee on their achievement.\", done_definition=\"Interviewer has finished discussing the candidate's past projects and their role in them.\", required=True)], <StageTypes.CODING: 'coding'>: [Step(name='describe_problem', description='Describe the problem to the candidate.', done_definition='Interviewer has finished describing the problem.', required=True), Step(name='prompt_coding', description='Let the candidate start writing the code to solve the problem.', done_definition='Interviewer has finished prompting the candidate to start writing the code, or the candidate has started writing the code without prompting.', required=True), Step(name='monitor_coding', description=\"Monitor the candidate's coding progress, respond accordingly if they need help, clarification, or if they are stuck and need hints.\", done_definition=\"Monitoring step isn't done until candidate has finished writing the complete code solution (not necessarily passed all test cases).\", required=True), Step(name='prompt_explain_code', description='Let the candidate explain their code and the approach they took to solve the problem.', done_definition='Interviewer has finished prompting the candidate to explain their code, or the candidate has finished explaining their code without prompting.', required=True), Step(name='prompt_test_case', description='Let the candidate write the test cases for their code.', done_definition='Interviewer has finished prompting the candidate to write the test cases, or the candidate has finished writing the test cases without prompting.', required=True), Step(name='prompt_optimization', description=\"If candidate's code can be optimized, ask them if they can think of a better solution\", done_definition='Interviewer has finished prompting the candidate to optimize their code.')], <StageTypes.EVAL: 'eval'>: [Step(name='tell_overall_perf', description='Tell candidate how they did in the interview.', done_definition='Interviewer has finished telling candidate how they did in the interview.', required=True), Step(name='tell_strength', description='Tell candidate what they did well and what they could improve on.', done_definition='Interviewer has finished telling candidate what they did well and what they could improve on.', required=True), Step(name='suggest_improve', description='Give candidate suggestions on how to improve their coding skills.', done_definition='Interviewer has finished giving candidate suggestions on how to improve their coding skills.', required=True), Step(name='ask_question_back', description='Ask candidate if they have any questions for you.', done_definition='Interviewer has finished asking candidate if they have any questions for you.', required=True), Step(name='answer_user_question', description='Answer related questions user might have', done_definition='Interviewer has finished answering related questions user might have')]}, 'signals': {<StageTypes.INTRO: 'intro'>: [Signal(name='candidate_intro_done', description='The candidate has introduced themselves about their background.'), Signal(name='candidate_goals_done', description='The candidate has finished talking about their career goals.'), Signal(name='candidate_projects_done', description='The candidate has finished talking about their past projects and their role in them.')], <StageTypes.CODING: 'coding'>: [Signal(name='clarifying_questions', description='The candidate asks clarifying questions about the problem.'), Signal(name='thought_process', description='The candidate explains their high-level thought process for solving the problem.'), Signal(name='code_finished', description='The candidate has finished writing the code.'), Signal(name='passed_all_tests', description='The candidate has passed all test cases.')], <StageTypes.EVAL: 'eval'>: [Signal(name='user_questions', description='User has asked questions about the interview process or the candidate.')]}, 'completed_steps': {<StageTypes.INTRO: 'intro'>: ['introduce_self', 'ask_background'], <StageTypes.CODING: 'coding'>: [], <StageTypes.EVAL: 'eval'>: []}, 'caught_signals': {<StageTypes.INTRO: 'intro'>: [], <StageTypes.CODING: 'coding'>: [], <StageTypes.EVAL: 'eval'>: []}}})\n",
      "('updates', {'decide_next_stage': {'trigger': False, 'current_stage': <StageTypes.INTRO: 'intro'>}})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in graph.astream(\n",
    "    input={\n",
    "        \"messages\": [\"Hi\"],\n",
    "        # \"event\": \"user_message\"\n",
    "        # \"trigger\": True,\n",
    "    },\n",
    "    config=config,\n",
    "    stream_mode=[\"values\", \"custom\"],\n",
    "):\n",
    "    # await graph.aupdate_state(config=config, values={ \"messages\": [\"hi\"]}, as_node=\"init_state\")\n",
    "    # state = await graph.aget_state(config=config)\n",
    "    # print(state)\n",
    "    print(chunk)\n",
    "\n",
    "# async for chunk in graph.astream(\n",
    "#     input={\n",
    "#         \"messages\": [\"Hi, my name is Brian. I am a software engineer working at Meta. My background is in computer science and I have a passion for building scalable and efficient systems. My previous projects are in building scalable and efficient systems, as well as building scalable and efficient systems. My goal is to build scalable and efficient systems.\"],\n",
    "#         # \"event\": \"user_message\"\n",
    "#         \"trigger\": True\n",
    "#     },\n",
    "#     config=config,\n",
    "#     stream_mode=[\"updates\"],\n",
    "# ):\n",
    "#     print(chunk)\n",
    "# list(graph.get_state_history(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load.load import load, loads\n",
    "from langchain_core.load.dump import dumpd, dumps\n",
    "from langgraph.types import StateSnapshot\n",
    "\n",
    "state = graph.get_state(config=config)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state = dumps(state)\n",
    "dumped_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state = loads(dumped_state, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_state_snapshot = StateSnapshot(*loaded_state)\n",
    "loaded_state_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot == state, dumpd(loaded_state_snapshot) == dumpd(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_state_snapshot.values == state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(graph.get_state_history(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_history = dumps(history)\n",
    "dumped_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history = loads(dumped_history, valid_namespaces=[\"agent_graph\"])\n",
    "loaded_history_snapshot = [StateSnapshot(*h) for h in loaded_history]\n",
    "loaded_history_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_history_snapshot == history, dumpd(loaded_history_snapshot) == dumpd(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_state_byte = dumped_state.encode()\n",
    "dumped_history_byte = dumped_history.encode()\n",
    "print(f\"State: {len(dumped_state_byte) / 1024:.2f} KB\")\n",
    "print(f\"State History: {len(dumped_history_byte) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
