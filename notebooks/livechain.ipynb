{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Annotated, Dict, List, Literal, cast\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from livechain.graph.executor import Workflow, WorkflowExecutor\n",
    "from livechain.graph.func import root, step, subscribe\n",
    "from livechain.graph.ops import channel_send, get_config, get_state, mutate_state, trigger_workflow\n",
    "from livechain.graph.types import EventSignal, TriggerSignal\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    messages: Annotated[List[AnyMessage], add_messages] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "AgentType = Literal[\"llm-researcher\", \"llm-engineer\", \"gen-ai-engineer\"]\n",
    "\n",
    "\n",
    "class AgentConfig(BaseModel):\n",
    "    agent_type: AgentType\n",
    "\n",
    "\n",
    "def agent_type_to_name(agent_type: AgentType) -> str:\n",
    "    if agent_type == \"llm-researcher\":\n",
    "        return \"Alex\"\n",
    "    elif agent_type == \"llm-engineer\":\n",
    "        return \"Bob\"\n",
    "    elif agent_type == \"gen-ai-engineer\":\n",
    "        return \"Charlie\"\n",
    "\n",
    "\n",
    "def agent_type_to_sys_prompt(agent_type: AgentType) -> str:\n",
    "    if agent_type == \"llm-researcher\":\n",
    "        prompt = (\n",
    "            \"You are a llm researcher, you work at OpenAI, your job involves building and training more advanced transformer models, \"\n",
    "            \"such as GPT-5. You are an expert behind math and various training algorithms such as GRPO, RLHF, MoE, etc. \"\n",
    "            \"You are invited to participate in a conversation with a llm engineer, and a gen ai product engineer to discuss the future of ai. \"\n",
    "        )\n",
    "    elif agent_type == \"llm-engineer\":\n",
    "        prompt = (\n",
    "            \"You are a llm engineer, you work at Meta, your job involves building and improving LLM serving frameworks and infrastructure, \"\n",
    "            \"such as vLLM, SGLang, etc. You are expert in writing different kind of CUDA kernels, quantizations, inference optimizations tricks \"\n",
    "            \"such as continuious batching, zero-overhead GPU CPU communication, efficient KV cache aware routings, etc. \"\n",
    "            \"You are invited to participate in a conversation with a llm researcher, and a gen ai product engineer to discuss the future of ai. \"\n",
    "        )\n",
    "    elif agent_type == \"gen-ai-engineer\":\n",
    "        prompt = (\n",
    "            \"You are a gen ai product engineer, you work at a Startup, your job involves building and improving gen ai products, \"\n",
    "            \"using LLM application frameworks such as LangChain, LangGraph, LlamaIndex, etc. You are an expert in prompt engineering, \"\n",
    "            \"RAG, agentic architectures, and LLM quality benchmarkings. You also have a great sense of product design and a deep understanding of user experience. \"\n",
    "            \"You are invited to participate in a conversation with a llm researcher, and a llm engineer to discuss the future of ai. \"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid agent type: {agent_type}\")\n",
    "\n",
    "    prompt += f\"Your name is {agent_type_to_name(agent_type)}. At the beginning of the conversation, you will introduce yourself to the other participants. \"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "class UserChatEvent(EventSignal):\n",
    "    agent_type: AgentType\n",
    "    message: str\n",
    "    should_speak: bool\n",
    "\n",
    "\n",
    "@step()\n",
    "async def chat():\n",
    "    state = get_state(AgentState)\n",
    "    config = get_config(AgentConfig)\n",
    "\n",
    "    sys_prompt = agent_type_to_sys_prompt(config.agent_type)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content=sys_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\"messages\": state.messages})\n",
    "    response.pretty_print()\n",
    "\n",
    "    await mutate_state({\"messages\": [response]})\n",
    "    await channel_send(\"chat\", {\"type\": config.agent_type, \"message\": response.content})\n",
    "\n",
    "\n",
    "@subscribe(UserChatEvent)\n",
    "async def on_user_chat(event: UserChatEvent):\n",
    "    name = agent_type_to_name(event.agent_type)\n",
    "    message = HumanMessage(content=f\"{name}: {event.message}\")\n",
    "    await mutate_state({\"messages\": [message]})\n",
    "\n",
    "    if event.should_speak:\n",
    "        await trigger_workflow()\n",
    "\n",
    "\n",
    "@root()\n",
    "async def entrypoint():\n",
    "    chat()\n",
    "\n",
    "\n",
    "wf = Workflow.from_nodes(entrypoint, [on_user_chat])\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent_executors: Dict[AgentType, WorkflowExecutor] = {}\n",
    "\n",
    "    for agent_type in [\"llm-researcher\", \"llm-engineer\", \"gen-ai-engineer\"]:\n",
    "        agent_type = cast(AgentType, agent_type)\n",
    "        executor = wf.compile(AgentState, config_schema=AgentConfig)\n",
    "        agent_executors[agent_type] = executor\n",
    "\n",
    "    keys: List[AgentType] = list(agent_executors.keys())\n",
    "    for agent_type in keys:\n",
    "        curr_agent_executor = agent_executors[agent_type]\n",
    "\n",
    "        @curr_agent_executor.recv(\"chat\")\n",
    "        async def on_chat(data: Dict):\n",
    "            agent_type = cast(AgentType, data[\"type\"])\n",
    "\n",
    "            for other_agent_type in keys:\n",
    "                if other_agent_type == agent_type:\n",
    "                    continue\n",
    "\n",
    "                # only next agent in the circle should speak\n",
    "                should_speak = (keys.index(agent_type) + 1) % len(keys) == keys.index(other_agent_type)\n",
    "\n",
    "                other_agent_executor = agent_executors[other_agent_type]\n",
    "                await other_agent_executor.publish_event(\n",
    "                    UserChatEvent(\n",
    "                        agent_type=data[\"type\"],\n",
    "                        message=data[\"message\"],\n",
    "                        should_speak=should_speak,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    for agent_type in keys:\n",
    "        executor = agent_executors[agent_type]\n",
    "        executor.start(config=AgentConfig(agent_type=agent_type))\n",
    "\n",
    "    first_agent_executor = agent_executors[keys[0]]\n",
    "    first_agent_executor.trigger_workflow(TriggerSignal())\n",
    "\n",
    "    while True:\n",
    "        await asyncio.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
